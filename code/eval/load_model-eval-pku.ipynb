{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "import tqdm\n",
    "\n",
    "model_id = \"MBZUAI/LaMini-GPT-124M\"\n",
    "peft_model_id = \"FXNan/gpt2-124M-DPO\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "model.load_adapter(peft_model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_id = \"PKU-Alignment/processed-hh-rlhf\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "ds = load_dataset(dataset_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_test = ds['test']\n",
    "print(ds_test)\n",
    "print(ds_test[0]['context'])\n",
    "print(ds_test[0]['chosen'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.chat_template = \"\"\"{{ \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\"}}\n",
    "{% for message in messages %}\n",
    "{% if message['role'] == 'human' %}\n",
    "{{ \"\\n\\n### Instruction:\" }}\n",
    "{{ message['text'] }}\n",
    "{% else %}\n",
    "{{ \"\\n\\n### Response:\" }}{{ message['text'] }}\n",
    "{% endif %}\n",
    "{% endfor %}\"\"\"\n",
    "\n",
    "print(tokenizer.apply_chat_template(ds_test[0]['context'], tokenize=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [x['context'] for x in ds_test]\n",
    "answers = [x['chosen'] for x in ds_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import Template\n",
    "\n",
    "model.eval()\n",
    "\n",
    "loss_array = []\n",
    "\n",
    "# for i in range(min(2000, len(questions))):\n",
    "for i in tqdm.tqdm(range(min(1000, len(questions)))):\n",
    "    q = questions[i]\n",
    "    q = tokenizer.apply_chat_template(q, tokenize=False) + \"\\n\\n### Response:\"\n",
    "    a = answers[i]['text']\n",
    "    qa = q\n",
    "    q_tokens = tokenizer(qa, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
    "    q_tokens_len = len(q_tokens['input_ids'][0])\n",
    "    tokens = tokenizer(qa+a, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
    "    qa_tokens_len = len(tokens['input_ids'][0])\n",
    "    a_tokens_len = qa_tokens_len - q_tokens_len\n",
    "    if a_tokens_len < 1:\n",
    "        print(f\"Skipping {i}, qa_tokens_len: {qa_tokens_len}, q_tokens_len: {q_tokens_len}, a_tokens_len: {a_tokens_len}\")\n",
    "        continue\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model(**tokens)\n",
    "    \n",
    "        logits = output['logits'][:, -a_tokens_len-1:-1, :]\n",
    "        \n",
    "        try:\n",
    "            loss = torch.nn.functional.cross_entropy(logits.view(-1, logits.shape[-1]), tokens['input_ids'][0, -a_tokens_len:].view(-1))\n",
    "        except:\n",
    "            print(f\"Error at {i}\")\n",
    "            print(f\"q_tokens_len: {q_tokens_len}\")\n",
    "            print(f\"qa_tokens_len: {qa_tokens_len}\")\n",
    "        loss_array.append(loss.item())\n",
    "        \n",
    "        \n",
    "print(f\"Mean loss: {sum(loss_array)/len(loss_array)}\")\n",
    "        \n",
    "# baseline: 2.8425033027483684\n",
    "# lora 5e-6: 2.8299622096741572\n",
    "# lora 1e-5: 2.7702952743533142"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
