{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "import tqdm\n",
    "\n",
    "model_id = \"MBZUAI/LaMini-GPT-124M\"\n",
    "peft_model_id = \"FXNan/gpt2-124M-DPO\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "model.load_adapter(peft_model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_id = \"truthfulqa/truthful_qa\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "ds = load_dataset(dataset_id, \"generation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = ds['validation']['question']\n",
    "answers = ds['validation']['best_answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import Template\n",
    "qa_template = \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\n${q}\\n\\n### Response:\"\n",
    "qa_template = Template(qa_template)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "loss_array = []\n",
    "\n",
    "# for i in range(min(2000, len(questions))):\n",
    "for i in tqdm.tqdm(range(len(questions))):\n",
    "    q = questions[i]\n",
    "    a = answers[i]\n",
    "    qa = qa_template.substitute(q=q)\n",
    "    q_tokens = tokenizer(qa, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
    "    q_tokens_len = len(q_tokens['input_ids'][0])\n",
    "    tokens = tokenizer(qa+a, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
    "    qa_tokens_len = len(tokens['input_ids'][0])\n",
    "    a_tokens_len = qa_tokens_len - q_tokens_len\n",
    "    if a_tokens_len < 1:\n",
    "        print(f\"Skipping {i}\")\n",
    "        continue\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model(**tokens)\n",
    "    \n",
    "        logits = output['logits'][:, -a_tokens_len-1:-1, :]\n",
    "        \n",
    "        try:\n",
    "            loss = torch.nn.functional.cross_entropy(logits.view(-1, logits.shape[-1]), tokens['input_ids'][0, -a_tokens_len:].view(-1))\n",
    "        except:\n",
    "            print(f\"Error at {i}\")\n",
    "            print(f\"q_tokens_len: {q_tokens_len}\")\n",
    "            print(f\"qa_tokens_len: {qa_tokens_len}\")\n",
    "        loss_array.append(loss.item())\n",
    "        \n",
    "        \n",
    "print(f\"Mean loss: {sum(loss_array)/len(loss_array)}\")\n",
    "        \n",
    "# baseline: 2.3752774173619553\n",
    "# lora 5e-6: 2.371942672636839\n",
    "# lora 1e-5: 2.3378114736977524"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
